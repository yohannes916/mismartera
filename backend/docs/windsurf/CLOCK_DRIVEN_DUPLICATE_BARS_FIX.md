# Clock-Driven Mode - Duplicate Bars Fix

**Date:** December 1, 2025  
**Issue:** Too many bars (2351 instead of ~173) and time ending ~4 seconds short  
**Root Cause:** Processing bars with timestamp == current_time instead of <= current_time

---

## ðŸ› **The Problem**

User reported:
```
1m Bars: 2351 bars | Quality: 100.0% | Start: 09:30 | Last: 12:23 | Span: 173min
```

**Expected:** ~173 bars for 173 minutes of 1m data  
**Actual:** 2351 bars (13.6x too many!)  

Also: Time ending ~4 seconds short of expected end time

---

## ðŸ” **Root Cause**

### **The Bug:**

Clock-driven streaming was calling `_process_queue_data_at_timestamp(current_time)` which processes bars with **exact timestamp match**:

```python
# session_coordinator.py line 2149
while queue and queue[0].timestamp == timestamp:  # EXACT match only!
    bar = queue.popleft()
```

### **What Was Happening:**

1. Clock advances: 09:30:00, 09:30:01, 09:30:02, ..., 09:30:59, 09:31:00
2. Each tick calls `_process_queue_data_at_timestamp(current_time)`
3. Bar at 09:30:00 gets processed when current_time == 09:30:00 âœ“
4. But the function was designed for data-driven mode (exact timestamp consumption)
5. In clock-driven mode, calling it repeatedly with advancing time caused issues

### **Why Too Many Bars:**

The function `_process_queue_data_at_timestamp` was being called from two places:
1. Clock-driven streaming (new code) - every second
2. Data-driven streaming (old code) - per data timestamp

This created a mismatch between comment and implementation:
- **Comment:** "Process all data with timestamp <= current_time"
- **Implementation:** `timestamp == current_time` (exact match)

---

## âœ… **The Fix**

Created a new helper function `_process_bars_up_to_time()` specifically for clock-driven mode:

```python
def _process_bars_up_to_time(coordinator, current_time):
    """Process all bars from queues with timestamp <= current_time.
    
    This is used by clock-driven mode to consume all pending bars as the
    clock advances, not just bars at exact timestamp matches.
    """
    bars_processed = 0
    
    for queue_key, queue in coordinator._bar_queues.items():
        symbol, interval = queue_key
        
        # Consume all bars up to current time (not just ==)
        while queue and queue[0].timestamp <= current_time:
            bar = queue.popleft()
            
            # Filter: Drop bars outside regular trading hours
            if market_open and market_close:
                if bar.timestamp < market_open or bar.timestamp > market_close:
                    continue  # Skip
            
            # Add to session data
            symbol_data.bars_base.append(bar)
            symbol_data.update_from_bar(bar)
            bars_processed += 1
            
            # Notify processors
            coordinator.data_processor.notify_data_available(...)
            coordinator.quality_manager.notify_data_available(...)
    
    return bars_processed
```

### **Key Changes:**

1. âœ… Uses `<= current_time` instead of `== current_time`
2. âœ… Processes ALL pending bars up to current time in one call
3. âœ… Filters out bars outside trading hours
4. âœ… Only consumes each bar **once** from queue
5. âœ… Separate implementation for clock-driven vs data-driven modes

---

## ðŸ“Š **Expected Behavior After Fix**

### **Clock-Driven Streaming:**

```
09:30:00 - Clock ticks to 09:30:00
         - Processes bars with timestamp <= 09:30:00
         - Might process: 09:30:00 (1 bar)

09:30:01 - Clock ticks to 09:30:01
         - Processes bars with timestamp <= 09:30:01
         - No new bars (next bar is at 09:31:00)

... (58 more ticks with no bars)

09:31:00 - Clock ticks to 09:31:00
         - Processes bars with timestamp <= 09:31:00
         - Processes: 09:31:00 (1 bar)
```

**Result:** Each bar processed **exactly once**, at or shortly after its timestamp

### **Bar Count:**

- 6.5 hour trading day = 390 minutes
- 1m bars = **~390 bars** (one per minute)
- Quality 100% = **390 bars** for full day

---

## ðŸŽ¯ **Why "4 Seconds Short"**

The timing issue was likely caused by the loop ending prematurely because:

1. Loop checks `if current_time >= market_close: break` 
2. But bars weren't being processed correctly
3. Queue might still have bars that weren't consumed
4. Loop ends before processing all bars

**After fix:** All bars with timestamp <= market_close will be processed before loop ends.

---

## ðŸ“ **Files Modified**

1. âœ… **`app/threads/streaming_modes.py`**
   - Added `_process_bars_up_to_time()` helper function
   - Clock-driven mode now uses this instead of `_process_queue_data_at_timestamp()`
   - Properly consumes bars with `timestamp <= current_time`
   - Filters out bars outside trading hours

---

## ðŸ”„ **Before vs After**

### **Before:**
```
Session: 09:30 to 12:23 (173 minutes)
Bars: 2351 bars
Result: 13.6x too many bars!
```

### **After (Expected):**
```
Session: 09:30 to 12:23 (173 minutes)
Bars: ~173 bars (1 per minute)
Result: Correct bar count âœ“
```

---

## ðŸ§ª **Testing**

```bash
./start_cli.sh
system start
data session

# Watch for:
# - Correct bar count (~173 for 173 minutes, ~390 for full day)
# - Time reaching expected end time (not 4 seconds short)
# - Quality staying at 100% (no duplicates or missing bars)
```

---

**Status:** âœ… Clock-driven bar processing fixed  
**Next:** Test to verify correct bar counts and timing
