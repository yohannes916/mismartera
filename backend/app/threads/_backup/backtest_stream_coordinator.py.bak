"""
================================================================================
BACKTEST STREAM COORDINATOR - Chronological Multi-Stream Merging & Time Control
================================================================================

OVERVIEW:
This module implements the central coordinator for backtest data streaming with
TWO CRITICAL RESPONSIBILITIES:

1. CHRONOLOGICAL MERGING
   - Merges multiple data streams (bars, ticks, quotes) across symbols
   - Yields data in perfect timestamp order (oldest-first)
   - No sorting overhead (data pre-sorted from database)
   - Thread-safe queue management for concurrent streams

2. TIME ADVANCEMENT (⚠️ CRITICAL ⚠️)
   - **ONLY component** that advances backtest time FORWARD
   - All other components only query or reset time
   - Respects bar intervals (1m bar @ 09:30 → time advances to 09:31)
   - Supports two modes: data-driven (fast) and clock-driven (realistic)

ARCHITECTURE:
┌─────────────────┐
│ SystemManager   │ ──> Provides TimeManager, mode checks, state checks
└─────────────────┘
         │
         ▼
┌────────────────────────────────────────────────────────────────────────┐
│                  BacktestStreamCoordinator                              │
│                                                                         │
│  Main Thread: Merge Worker (_merge_worker)                             │
│  ├─ Fetch next item from each stream                                   │
│  ├─ Find oldest timestamp across all streams                           │
│  ├─ Filter out-of-hours data (pre-market, after-hours)                │
│  ├─ ADVANCE TIME to match data (ONLY place this happens) ⚠️           │
│  ├─ Write data to SessionData                                          │
│  └─ Yield to output queue                                              │
│                                                                         │
│  Clock Thread: Clock Worker (_clock_worker) [speed > 0 only]          │
│  └─ Advances time independently at specified speed                     │
│                                                                         │
│  Two Modes:                                                             │
│  • Data-Driven (speed=0): Stream immediately, advance time to data     │
│  • Clock-Driven (speed>0): Wait for time to reach data, then stream   │
└────────────────────────────────────────────────────────────────────────┘
         │                           │
         │ (writes bars)             │ (provides data)
         ▼                           ▼
┌──────────────────┐      ┌──────────────────────────────┐
│  SessionData     │      │  DataManager                  │
│  (Thread-safe)   │      │  (Registers streams, feeds)  │
└──────────────────┘      └──────────────────────────────┘
         │
         │ (signals new data)
         ▼
┌──────────────────┐
│ DataUpkeepThread │
│ (Quality, Gaps)  │
└──────────────────┘

THREADING MODEL:
- **Merge Worker**: Dedicated daemon thread for stream processing
- **Clock Worker**: Optional daemon thread for time advancement (speed > 0)
- **Thread-safe**: All queues and shared state protected by locks
- **Independent**: Runs separately from CLI event loop for consistent timing

TIME ADVANCEMENT RULES (⚠️ CRITICAL ⚠️):
1. **ONLY this coordinator advances time FORWARD**
2. Bar timestamps lag by interval duration:
   - 1m bar @ 09:30:00 = interval [09:30:00 - 09:30:59]
   - Time set to 09:31:00 when yielding (bar complete)
3. DataUpkeepThread can reset time to market open (day transitions)
4. All other components only QUERY time, never set it

MARKET HOURS FILTERING:
- Pre-market data (before 09:30): Discarded
- After-hours data (after 16:01): Discarded
- Future day data: Preserved in queue for next day
- Date-aware: Only filters current trading day

OPERATING MODES:
1. **Data-Driven (speed = 0)**
   - Streams as fast as consumers can process
   - Time advances immediately when yielding data
   - No independent clock thread
   - Use for: Fast backtesting, testing, debugging

2. **Clock-Driven (speed > 0)**
   - Clock thread advances time independently
   - Worker waits for time to reach data before streaming
   - Realistic timing simulation
   - Use for: Strategy testing with timing constraints

CREATION & LIFECYCLE:
1. Created by: DataManager via get_coordinator() singleton
2. Started by: coordinator.start_worker()
3. Registers streams: coordinator.register_stream(symbol, type)
4. Feeds data: coordinator.feed_data_list(symbol, type, data)
5. Runs until: stop_worker() called or system shutdown
6. Main methods: _merge_worker() and _clock_worker()

For detailed analysis, see: /backend/BACKTEST_STREAM_COORDINATOR_ANALYSIS.md
================================================================================
"""
from __future__ import annotations

import heapq
import queue
import threading
import time
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Iterator, Any, Dict, Tuple, Optional, List

from app.config import settings
from app.logger import logger
from app.models.database import SessionLocal


class StreamType(Enum):
    """Types of market data streams."""
    BAR = "bar"
    TICK = "tick"
    QUOTE = "quote"


@dataclass(order=True)
class StreamItem:
    """Wrapper for stream data with timestamp for priority queue ordering."""
    timestamp: datetime
    data: Any = field(compare=False)
    stream_key: Tuple[str, StreamType] = field(compare=False)


class BacktestStreamCoordinator:
    """Central coordinator for backtest data streams.
    
    Manages multiple concurrent backtest streams (bars, ticks, quotes) across
    different symbols and merges them in chronological order. Prevents duplicate
    streams and handles stream lifecycle.
    
    ARCHITECTURE:
    - Requires SystemManager reference for operation mode and state management
    - Uses SystemManager as single source of truth (no settings fallback)
    - Pauses time advancement when SystemManager.is_running() is False
    
    This coordinator is responsible for advancing backtest time as data flows
    through it. It is the ONLY component that should advance time forward in
    backtest mode.
    
    Thread-safe for concurrent registration/deregistration.
    """
    
    def __init__(self, system_manager=None, data_manager=None, data_repository=None):
        """Initialize the coordinator.
        
        Args:
            system_manager: Reference to SystemManager (REQUIRED for production use)
                           Without SystemManager, coordinator cannot determine mode
                           or respect system state (paused/running/stopped)
            data_manager: Reference to DataManager for prefetch worker
            data_repository: Optional data repository for gap filling
        
        Uses TimeManager (via SystemManager) to ensure time synchronization
        across all components (DataManager, streams, etc.).
        
        Uses a dedicated worker thread for processing that runs independently
        of the asyncio event loop.
        
        Phase 2: Also initializes DataUpkeepThread for background data maintenance.
        """
        # Reference to SystemManager for checking state
        self._system_manager = system_manager
        self._data_manager = data_manager
        self._data_repository = data_repository
        
        if system_manager is None:
            logger.warning(
                "BacktestStreamCoordinator initialized without SystemManager - "
                "mode checks and state management will not work properly!"
            )
        
        # Active streams: (symbol, stream_type) -> queue.Queue (thread-safe)
        self._active_streams: Dict[Tuple[str, StreamType], queue.Queue] = {}
        
        # Track timestamps for queued items: (symbol, stream_type) -> {"oldest": datetime, "newest": datetime}
        self._queue_timestamps: Dict[Tuple[str, StreamType], Dict[str, Optional[datetime]]] = {}
        
        # Lock for thread-safe stream registration
        self._lock = threading.Lock()
        
        # Priority queue for merging streams: (timestamp, stream_key, data)
        self._merge_heap: List[StreamItem] = []
        
        # Worker thread for processing streams
        self._worker_thread: Optional[threading.Thread] = None
        
        # Clock thread for continuous time advancement (speed > 0)
        self._clock_thread: Optional[threading.Thread] = None
        self._clock_start_time: Optional[float] = None  # Wall-clock start (time.time())
        self._backtest_start_time: Optional[datetime] = None  # Backtest time start
        self._clock_paused = threading.Event()  # Pause clock when system paused
        self._clock_paused.set()  # Start paused
        
        # Pending items from merge worker (for accurate queue peeking)
        self._pending_items: Optional[Dict[Tuple[str, StreamType], Optional[StreamItem]]] = None
        
        # Output queue for merged stream (thread-safe)
        self._output_queue: queue.Queue = queue.Queue()
        
        # Shutdown flag (thread-safe)
        self._shutdown = threading.Event()
        
        # Time manager for advancing backtest time (singleton shared with system)
        # Access via system_manager as single source of truth
        self._time_manager = system_manager.get_time_manager() if system_manager else None
        
        # Session data for storing streamed data
        from app.managers.data_manager.session_data import get_session_data
        self._session_data = get_session_data()
        
        # Note: Data-upkeep thread is now managed by SystemManager for entire system lifecycle
        # It no longer lives in the stream coordinator
        
        # Track last timestamp for pacing
        self._last_timestamp: Optional[datetime] = None
        
        sys_mgr_status = "with SystemManager" if system_manager is not None else "WITHOUT SystemManager"
        logger.info(f"BacktestStreamCoordinator initialized with threading ({sys_mgr_status})")
    
    def register_stream(
        self,
        symbol: str,
        stream_type: StreamType,
    ) -> Tuple[bool, Optional[queue.Queue]]:
        """Register a new stream for a symbol and data type.
        
        Args:
            symbol: Stock symbol (e.g., "AAPL")
            stream_type: Type of data stream
            
        Returns:
            Tuple of (success: bool, queue: Optional[queue.Queue])
            If successful, returns (True, queue) where queue is the thread-safe
            input queue for this stream. If stream already exists, returns (False, None).
        """
        stream_key = (symbol.upper(), stream_type)
        
        with self._lock:
            if stream_key in self._active_streams:
                logger.warning(
                    f"Stream already active for {stream_key[0]} ({stream_key[1].value})"
                )
                return False, None
            
            # Create thread-safe input queue for this stream
            input_queue = queue.Queue()
            self._active_streams[stream_key] = input_queue
            
            # Initialize timestamp tracking
            self._queue_timestamps[stream_key] = {"oldest": None, "newest": None}
            
            logger.info(
                f"Registered stream for {stream_key[0]} ({stream_key[1].value})"
            )
            
            return True, input_queue
    
    def feed_data_list(
        self,
        symbol: str,
        stream_type: StreamType,
        data_list: list,
    ) -> bool:
        """Feed a pre-fetched list of data directly into the stream queue.
        
        This is faster than async iteration and can be called from any thread.
        Use this when you've already fetched all data from DB.
        
        Args:
            symbol: Stock symbol
            stream_type: Type of data stream
            data_list: List of data objects (BarData, TickData, etc.)
            
        Returns:
            True if successful, False if stream not registered
        """
        stream_key = (symbol.upper(), stream_type)
        
        with self._lock:
            input_queue = self._active_streams.get(stream_key)
        
        if input_queue is None:
            logger.error(f"Cannot feed data for {stream_key} - stream not registered")
            return False
        
        # Feed all data directly into queue (fast, thread-safe)
        # Track the newest timestamp in this batch
        batch_newest_ts = None
        
        for data in data_list:
            input_queue.put(data)
            
            # Track newest timestamp in this batch
            if hasattr(data, 'timestamp'):
                ts = data.timestamp
                if batch_newest_ts is None or ts > batch_newest_ts:
                    batch_newest_ts = ts
        
        # Update newest timestamp tracking (keep the maximum across all batches)
        with self._lock:
            if stream_key in self._queue_timestamps:
                existing = self._queue_timestamps[stream_key]
                current_newest = existing.get("newest")
                
                # Keep the maximum (newest) timestamp seen across all batches
                if batch_newest_ts:
                    if current_newest is None or batch_newest_ts > current_newest:
                        self._queue_timestamps[stream_key]["newest"] = batch_newest_ts
        
        # Signal end of stream
        input_queue.put(None)
        
        logger.info(f"Fed {len(data_list)} items to stream {stream_key}")
        return True
    
    def deregister_stream(
        self,
        symbol: str,
        stream_type: StreamType,
    ) -> bool:
        """Deregister a stream (called when stream is exhausted).
        
        Args:
            symbol: Stock symbol
            stream_type: Type of data stream
            
        Returns:
            True if stream was found and removed, False otherwise
        """
        stream_key = (symbol.upper(), stream_type)
        
        with self._lock:
            if stream_key in self._active_streams:
                del self._active_streams[stream_key]
                # Also remove timestamp tracking
                if stream_key in self._queue_timestamps:
                    del self._queue_timestamps[stream_key]
                logger.info(
                    f"Deregistered stream for {stream_key[0]} ({stream_key[1].value})"
                )
                return True
            
            return False
    
    def is_stream_active(
        self,
        symbol: str,
        stream_type: StreamType,
    ) -> bool:
        """Check if a stream is currently active.
        
        Args:
            symbol: Stock symbol
            stream_type: Type of data stream
            
        Returns:
            True if stream is active, False otherwise
        """
        stream_key = (symbol.upper(), stream_type)
        with self._lock:
            return stream_key in self._active_streams
    
    def get_queue_stats(self) -> Dict[str, Dict[str, Dict[str, Any]]]:
        """Get queue statistics for all active streams, grouped by interval.
        
        Returns:
            Dictionary mapping symbol to interval to stats dict
            Stats dict contains: {"size": int, "oldest": datetime, "newest": datetime}
            Example: {"AAPL": {"BAR": {"size": 150, "oldest": datetime(...), "newest": datetime(...)}}}
            
            For BAR streams, interval is shown as "BAR"
            For other streams (TICK, QUOTE), interval is the stream type name
            
            Note: 
            - oldest is from pending_items (accurate: next item to be consumed)
            - newest is from initial batch timestamps (last item in original batch)
            - size is current queue size
        """
        stats = {}
        with self._lock:
            for (symbol, stream_type), input_queue in self._active_streams.items():
                if symbol not in stats:
                    stats[symbol] = {}
                
                queue_size = input_queue.qsize()
                interval_key = "BAR"  # Default for bar type
                
                if stream_type != StreamType.BAR:
                    interval_key = stream_type.value.upper()
                
                # Get ACTUAL oldest timestamp from pending_items (worker's staging area)
                oldest_ts = None
                if self._pending_items:
                    stream_key = (symbol, stream_type)
                    pending_item = self._pending_items.get(stream_key)
                    if pending_item and hasattr(pending_item, 'timestamp'):
                        oldest_ts = pending_item.timestamp
                
                # Get newest timestamp from initial batch tracking
                timestamps = self._queue_timestamps.get((symbol, stream_type), {})
                newest_ts = timestamps.get("newest")
                
                # Use interval as key (e.g., "BAR", "TICK", "QUOTE")
                if interval_key not in stats[symbol]:
                    stats[symbol][interval_key] = {
                        "size": 0,
                        "oldest": None,
                        "newest": None
                    }
                
                # Add queue size (items still in queue + 1 in pending)
                total_items = queue_size + (1 if oldest_ts else 0)
                stats[symbol][interval_key]["size"] += total_items
                
                # Update timestamps if available
                if oldest_ts:
                    if stats[symbol][interval_key]["oldest"] is None or oldest_ts < stats[symbol][interval_key]["oldest"]:
                        stats[symbol][interval_key]["oldest"] = oldest_ts
                if newest_ts:
                    if stats[symbol][interval_key]["newest"] is None or newest_ts > stats[symbol][interval_key]["newest"]:
                        stats[symbol][interval_key]["newest"] = newest_ts
        
        return stats
    
    def _clock_worker(self) -> None:
        """Background clock thread for continuous time advancement (speed > 0).
        
        This thread runs independently and advances backtest time based on
        wall-clock elapsed time multiplied by the speed setting.
        
        Only active when DATA_MANAGER_BACKTEST_SPEED > 0.
        """
        try:
            speed = settings.DATA_MANAGER_BACKTEST_SPEED
            
            if speed <= 0:
                logger.warning("Clock worker started but speed <= 0, exiting")
                return
            
            logger.info(f"Clock worker started (speed: {speed}x)")
            
            while not self._shutdown.is_set():
                # Wait for system to be running
                if not self._system_manager.is_running():
                    self._clock_paused.clear()
                    time.sleep(0.1)
                    continue
                
                # Mark clock as running
                self._clock_paused.set()
                
                # Initialize clock on first run after becoming active
                if self._clock_start_time is None:
                    try:
                        self._backtest_start_time = self._time_manager.get_current_time()
                        self._clock_start_time = time.time()
                        logger.info(f"Clock initialized: backtest_time={self._backtest_start_time}")
                    except ValueError:
                        # Backtest time not set yet, wait
                        time.sleep(0.1)
                        continue
                
                # Calculate elapsed time and advance backtest time
                wall_elapsed = time.time() - self._clock_start_time
                backtest_elapsed_seconds = wall_elapsed * speed
                new_backtest_time = self._backtest_start_time + timedelta(seconds=backtest_elapsed_seconds)
                
                # Set the new time
                self._time_manager.set_backtest_time(new_backtest_time)
                
                # Sleep for 10ms (100 Hz update rate)
                time.sleep(0.01)
        
        except Exception as exc:
            logger.error(f"Clock worker error: {exc}", exc_info=True)
        finally:
            logger.info("Clock worker thread stopped")
    
    def start_worker(self) -> None:
        """Start the background worker thread that merges streams chronologically.
        
        The thread runs independently of the CLI event loop, providing consistent
        timing for backtest streaming.
        
        Phase 2: Also starts the data-upkeep thread for background data maintenance.
        Phase 3: Starts clock thread for continuous time advancement (speed > 0).
        """
        if self._worker_thread is not None and self._worker_thread.is_alive():
            logger.warning("Worker thread already running")
            return
        
        self._shutdown.clear()
        self._worker_thread = threading.Thread(
            target=self._merge_worker,
            name="BacktestStreamWorker",
            daemon=True  # Don't prevent program exit
        )
        self._worker_thread.start()
        logger.info("Started backtest stream merge worker thread")
        
        # Start clock thread if speed > 0 (clock-driven mode)
        speed = settings.DATA_MANAGER_BACKTEST_SPEED
        if speed > 0:
            if self._clock_thread is None or not self._clock_thread.is_alive():
                self._clock_thread = threading.Thread(
                    target=self._clock_worker,
                    name="BacktestClockWorker",
                    daemon=True
                )
                self._clock_thread.start()
                logger.info(f"Started backtest clock thread (speed: {speed}x)")
            else:
                logger.warning("Clock thread already running")
        else:
            logger.info("Speed = 0: data-driven mode (no independent clock)")
    
    def stop_worker(self) -> None:
        """Stop the background worker thread gracefully.
        
        Also stops the clock thread for time advancement.
        Note: Data-upkeep thread is managed by SystemManager, not here.
        """
        
        # Signal shutdown to all threads
        self._shutdown.set()
        
        # Stop clock thread (Phase 3)
        if self._clock_thread is not None and self._clock_thread.is_alive():
            self._clock_thread.join(timeout=2.0)
            if self._clock_thread.is_alive():
                logger.warning("Clock thread did not stop gracefully")
            else:
                logger.info("Stopped backtest clock thread")
            self._clock_thread = None
            # Reset clock state
            self._clock_start_time = None
            self._backtest_start_time = None
        
        # Stop merge worker thread
        if self._worker_thread is None or not self._worker_thread.is_alive():
            return
        
        # Signal all input queues to unblock worker
        with self._lock:
            for q in self._active_streams.values():
                # Put None as sentinel to unblock queue.get()
                try:
                    q.put_nowait(None)
                except queue.Full:
                    pass
        
        # Wait for thread to finish (up to 5 seconds)
        self._worker_thread.join(timeout=5.0)
        if self._worker_thread.is_alive():
            logger.warning("Worker thread did not stop gracefully")
        else:
            logger.info("Stopped backtest stream merge worker thread")
        
        self._worker_thread = None
    
    def _merge_worker(self) -> None:
        """
        ========================================================================
        MERGE WORKER - Chronological Stream Merging & Time Advancement
        ========================================================================
        
        This is the MAIN thread responsible for:
        1. Merging multiple data streams in chronological order
        2. Advancing backtest time (ONLY place this happens)
        3. Filtering out-of-hours data
        4. Writing data to session_data
        5. Yielding merged output
        
        Loop Structure:
        ├─ STEP 1: Check Market Close
        │  └─> Pause if past market close + 1min
        ├─ STEP 2: Fetch Next Items
        │  └─> Pull from each stream, skip stale data
        ├─ STEP 3: Find Oldest
        │  └─> Scan pending items for oldest timestamp
        ├─ STEP 4: Filter Hours
        │  └─> Discard pre-market/after-hours for current day
        ├─ STEP 5: Advance Time (⚠️ CRITICAL)
        │  └─> Set time to bar end (data-driven) or wait for clock (clock-driven)
        ├─ STEP 6: Write Data
        │  └─> Store in session_data, update quality
        └─ STEP 7: Yield Output
           └─> Put in output queue, mark as consumed
        
        This runs in a dedicated daemon thread for precise timing control.
        Thread-safe via queue operations and locks.
        ========================================================================
        """
        logger.info("Merge worker thread started")
        
        # ====================================================================
        # PENDING ITEMS - Staging Area for Chronological Merging
        # ====================================================================
        # This dict serves as our "peek" mechanism:
        # - Pull one item from each stream queue
        # - Keep it here until it's the oldest across ALL streams
        # - Then yield it and mark as None to fetch next
        # 
        # Key: (symbol, stream_type)
        # Value: StreamItem or None (None = need to fetch next)
        # ====================================================================
        pending_items: Dict[Tuple[str, StreamType], Optional[StreamItem]] = {}
        
        # Store reference so get_queue_stats can access it for display
        self._pending_items = pending_items
        
        try:
            while not self._shutdown.is_set():
                # ============================================================
                # STEP 1: CHECK MARKET CLOSE
                # ============================================================
                # When: Market close + 1min buffer reached
                # Action: Pause streaming, wait for upkeep thread to handle EOD
                # Purpose: Prevents streaming after-hours data
                # ============================================================
                if self._system_manager.is_backtest_mode():
                    try:
                        # Get current time in system timezone
                        current_time = self._time_manager.get_current_time()
                        current_date = current_time.date()
                        
                        # Check market close (sync)
                        def check_market_close():
                            with SessionLocal() as db_session:
                                trading_session = self._time_manager.get_trading_session(
                                    db_session, current_date
                                )
                                
                                if trading_session and not trading_session.is_holiday:
                                    # Get close time in system timezone from TradingSession
                                    close_time = trading_session.get_regular_close_datetime()
                                    if close_time is None:
                                        return False
                                    
                                    # Add 1 minute buffer to ensure we process through the close minute
                                    # E.g., if close is 16:00, continue until 16:01 to capture all close data
                                    close_time_with_buffer = close_time + timedelta(minutes=1)
                                    return current_time >= close_time_with_buffer
                                return False
                        
                        market_closed = check_market_close()
                        if market_closed:
                            logger.debug(f"Market close reached at {current_time}, waiting")
                            time.sleep(0.1)
                            continue
                    except Exception as e:
                        logger.debug(f"Error checking market close: {e}")
                
                # ============================================================
                # STEP 2: FETCH NEXT ITEMS FROM STREAMS
                # ============================================================
                # For each stream without pending data:
                # 1. Pull next item from queue (blocking with timeout)
                # 2. Skip stale data (older than current time)
                # 3. Handle stream exhaustion (None sentinel)
                # 4. Store in pending_items for comparison
                # ============================================================
                # Get snapshot of active streams
                with self._lock:
                    active_keys = list(self._active_streams.keys())
                
                if not active_keys:
                    # No active streams, wait a bit before checking again
                    time.sleep(0.1)
                    continue
                
                # Fetch next item from each stream that doesn't have pending data
                for stream_key in active_keys:
                    if stream_key in pending_items and pending_items[stream_key] is not None:
                        continue
                    
                    with self._lock:
                        q = self._active_streams.get(stream_key)
                    
                    if q is None:
                        continue
                    
                    try:
                        # Keep fetching from this stream until we get data >= current time
                        while True:
                            # Blocking get with timeout
                            data = q.get(timeout=0.1)
                            
                            # None is sentinel for stream exhaustion
                            if data is None:
                                pending_items[stream_key] = None
                                self.deregister_stream(stream_key[0], stream_key[1])
                                break
                            
                            # Extract timestamp (works for BarData, TickData, quotes)
                            if hasattr(data, 'timestamp'):
                                ts = data.timestamp
                            else:
                                logger.warning(f"Data missing timestamp attribute: {data}")
                                continue
                            
                            # Skip data older than current backtest time
                            # This handles cases where a new stream is registered mid-backtest
                            try:
                                current_time = self._time_manager.get_current_time()
                                
                                # Both timestamps should be in system timezone
                                # Simple comparison - no timezone conversion needed
                                if ts < current_time:
                                    logger.debug(
                                        f"Skipping stale data from {stream_key}: "
                                        f"{ts} < current time {current_time}"
                                    )
                                    continue  # Fetch next item from this stream
                            except ValueError:
                                # Backtest time not set yet (initial state), accept all data
                                pass
                            
                            # Data is current or future, accept it
                            pending_items[stream_key] = StreamItem(
                                timestamp=ts,
                                data=data,
                                stream_key=stream_key,
                            )
                            break  # Exit the while loop, move to next stream
                        
                    except queue.Empty:
                        # No data available yet, will try again next iteration
                        continue
                
                # ============================================================
                # STEP 3: FIND OLDEST TIMESTAMP
                # ============================================================
                # Scan all pending items to find the oldest timestamp
                # This ensures chronological order across all streams
                # Simple comparison (all times in system timezone)
                # ============================================================
                oldest_item: Optional[StreamItem] = None
                oldest_key: Optional[Tuple[str, StreamType]] = None
                
                for stream_key, item in pending_items.items():
                    if item is None:
                        continue
                    if oldest_item is None or item.timestamp < oldest_item.timestamp:
                        oldest_item = item
                        oldest_key = stream_key
                
                # ============================================================
                # STEP 4: FILTER MARKET HOURS
                # ============================================================
                # For oldest item, check if within trading hours:
                # - Only filter CURRENT day data
                # - Pre-market (< 09:30): Discard
                # - After-hours (> 16:01): Discard
                # - Future day data: Preserve in queue
                # ============================================================
                if oldest_item is not None and oldest_key is not None:
                    # This prevents pre-market and after-hours data from being processed
                    # But preserves future trading day data that has been prefetched
                    if self._system_manager and self._system_manager.is_backtest_mode():
                        try:
                            from app.models.database import SessionLocal
                            current_time = self._time_manager.get_current_time()
                            current_date = current_time.date()
                            bar_date = oldest_item.timestamp.date()
                            
                            # Only check hours if bar is from the CURRENT trading day
                            # Future day data should be kept in queue for later
                            if bar_date == current_date:
                                with SessionLocal() as db_session:
                                    trading_session = self._time_manager.get_trading_session(db_session, current_date)
                                    if trading_session:
                                        open_time = trading_session.get_regular_open_datetime()
                                        close_time = trading_session.get_regular_close_datetime()
                                        # Add 1-minute buffer to allow close-of-day data
                                        from datetime import timedelta
                                        close_time_with_buffer = close_time + timedelta(minutes=1)
                                        
                                        # If bar timestamp is before market open, discard it
                                        if oldest_item.timestamp < open_time:
                                            logger.debug(
                                                f"Discarding pre-market data from {oldest_key}: "
                                                f"{oldest_item.timestamp} < open {open_time}"
                                            )
                                            # Clear this pending item and fetch next
                                            pending_items[oldest_key] = None
                                            continue
                                        
                                        # If bar timestamp is past market close + buffer, discard it
                                        if oldest_item.timestamp > close_time_with_buffer:
                                            logger.debug(
                                                f"Discarding after-hours data from {oldest_key}: "
                                                f"{oldest_item.timestamp} > close {close_time_with_buffer}"
                                            )
                                            # Clear this pending item and fetch next
                                            pending_items[oldest_key] = None
                                            continue
                        except Exception as e:
                            # If check fails, log and continue processing (fail-safe)
                            logger.debug(f"Error checking market hours for data filtering: {e}")
                    
                    # ============================================================
                    # STEP 5: ADVANCE TIME (⚠️ CRITICAL - ONLY PLACE)
                    # ============================================================
                    # This is the ONLY place where backtest time moves FORWARD
                    # Bar timestamps lag by interval:
                    #   - 1m bar @ 09:30:00 = interval [09:30:00 - 09:30:59]
                    #   - Target time: 09:31:00 (end of interval)
                    # Two modes:
                    #   - Data-driven (speed=0): Advance immediately
                    #   - Clock-driven (speed>0): Wait for clock to reach target
                    # ============================================================
                    
                    # SystemManager is REQUIRED for mode and state checks
                    if self._system_manager is None:
                        logger.error("SystemManager not available in BacktestStreamCoordinator - cannot determine mode")
                        # Without SystemManager, we cannot safely proceed
                        # Just yield data without time advancement or state checks
                        self._output_queue.put(oldest_item)
                        continue
                    
                    # Check if we're in backtest mode
                    mode_is_backtest = self._system_manager.is_backtest_mode()
                    
                    if mode_is_backtest:
                        # In backtest mode, check system state before advancing
                        # If system is not running, pause and wait
                        while not self._system_manager.is_running() and not self._shutdown.is_set():
                            # System is paused or stopped, wait for it to resume
                            time.sleep(0.1)
                            # Check if we should shutdown
                            if self._shutdown.is_set():
                                return
                        
                        # HYBRID TIME ADVANCEMENT
                        # - Speed > 0 (clock-driven): Wait for time to reach data, then stream
                        # - Speed = 0 (data-driven): Stream immediately, advance time to data
                        
                        speed = settings.DATA_MANAGER_BACKTEST_SPEED
                        
                        # Determine the target timestamp based on data type and interval
                        # For BARS: timestamp represents the START of the interval
                        #   so target is END of interval to represent "bar is complete"
                        #   - 1s bar: 09:30:00 = interval [09:30:00.000 - 09:30:00.999], target = 09:30:01
                        #   - 1m bar: 09:30:00 = interval [09:30:00 - 09:30:59], target = 09:31:00
                        # For QUOTES/TICKS: timestamp is the exact event time, use as-is
                        stream_type = oldest_key[1]
                        if stream_type == StreamType.BAR:
                            # Get interval from bar data to determine target time
                            bar_interval = oldest_item.data.interval
                            
                            if bar_interval == "1s":
                                # Add 1 second to get end of 1s bar interval
                                target_time = oldest_item.timestamp + timedelta(seconds=1)
                            elif bar_interval == "1m":
                                # Add 1 minute to get end of 1m bar interval
                                target_time = oldest_item.timestamp + timedelta(minutes=1)
                            else:
                                # Derived bars shouldn't flow through coordinator, but handle gracefully
                                logger.warning(f"Unexpected bar interval in coordinator: {bar_interval}")
                                target_time = oldest_item.timestamp + timedelta(minutes=1)
                        else:
                            # Quote/tick: use exact timestamp
                            target_time = oldest_item.timestamp
                        
                        if speed > 0:
                            # CLOCK-DRIVEN MODE (speed > 0)
                            # Clock thread advances time independently
                            # We wait until backtest time reaches the data timestamp, then stream it
                            
                            while not self._shutdown.is_set():
                                try:
                                    current_time = self._time_manager.get_current_time(timezone="UTC")
                                    
                                    # Check if time has reached or passed the target time
                                    if current_time >= target_time:
                                        # Time has caught up, stream this data
                                        break
                                    
                                    # Time hasn't reached data yet, wait briefly
                                    time.sleep(0.01)  # 10ms polling
                                    
                                except ValueError:
                                    # Backtest time not initialized yet, wait
                                    time.sleep(0.01)
                                    continue
                            
                            # Check if we're shutting down
                            if self._shutdown.is_set():
                                return
                        
                        else:
                            # DATA-DRIVEN MODE (speed = 0)
                            # No independent clock - advance time as fast as consumers can keep up
                            # Stream data immediately, then advance time to match it
                            
                            # TODO: Wait for consumer synchronization signals here
                            # - Upkeep thread: derivatives computed
                            # - Analysis engine: data consumed
                            # For now, advance immediately (fast as possible)
                            
                            self._time_manager.set_backtest_time(target_time)
                    
                    # ============================================================
                    # STEP 6: WRITE DATA TO SESSION_DATA
                    # ============================================================
                    # Store bar in session_data for:
                    # - AnalysisEngine consumption
                    # - Quality calculation
                    # - Historical access
                    # ============================================================
                    symbol = oldest_key[0]
                    stream_type = oldest_key[1]
                    
                    if stream_type == StreamType.BAR:
                        # Write bar to session_data (async operation run in thread)
                        try:
                            # Add bar directly (now sync)
                            self._session_data.add_bar(symbol, oldest_item.data)
                            
                            # Update quality immediately after consuming fundamental data
                            self._update_quality_for_symbol(symbol)
                        except Exception as e:
                            logger.error(f"Failed to write bar to session_data: {e}")
                    
                    # ============================================================
                    # STEP 7: YIELD OUTPUT
                    # ============================================================
                    # Put data in output queue for consumers
                    # Mark pending item as consumed (None = fetch next)
                    # ============================================================
                    self._output_queue.put(oldest_item.data)
                    # Mark this stream as needing new data
                    pending_items[oldest_key] = None
                else:
                    # No data available from any stream, wait a bit
                    time.sleep(0.01)
        
        except Exception as exc:
            logger.error(f"Merge worker error: {exc}", exc_info=True)
        finally:
            # Signal output queue is done
            self._output_queue.put(None)
            logger.info("Merge worker thread stopped")
    
    def get_merged_stream(self) -> Iterator[Any]:
        """Get the merged output stream in chronological order.
        
        Yields data from all active streams in timestamp order (oldest first).
        Continues until all streams are exhausted.
        
        Reads from the thread-safe output queue populated by the worker thread.
        
        Yields:
            Data objects (BarData, TickData, or quote objects) in chronological order
        """
        while True:
            # Read from thread-safe queue
            data = self._output_queue.get()
            if data is None:
                # Sentinel for end of stream
                break
            yield data
    
    def _update_quality_for_symbol(self, symbol: str) -> None:
        """Update bar quality immediately after consuming a fundamental bar.
        
        This provides real-time quality updates instead of waiting for the
        periodic upkeep thread cycle.
        
        Args:
            symbol: Symbol to update quality for
        """
        try:
            symbol_data = self._session_data.get_symbol_data(symbol)
            if not symbol_data or len(symbol_data.bars_1m) == 0:
                return
            
            # Get current time from TimeManager (single source of truth)
            if not self._time_manager:
                return
            
            try:
                current_time = self._time_manager.get_current_time(timezone="UTC")
            except ValueError:
                return  # Backtest time not set yet
            
            # Get trading session to determine session start
            current_date = current_time.date()
            from app.models.database import SessionLocal
            
            with SessionLocal() as session:
                trading_session = self._time_manager.get_trading_session(session, current_date)
                
                if not trading_session or trading_session.is_holiday:
                    return
                
                # Get session start time in UTC from TradingSession
                session_start_time = trading_session.get_regular_open_utc()
                if session_start_time is None:
                    return
            
            # Use detailed gap analysis
            from app.managers.data_manager.gap_detection import detect_gaps
            bars_1m = list(symbol_data.bars_1m)
            gaps = detect_gaps(
                symbol=symbol,
                session_start=session_start_time,
                current_time=current_time,
                existing_bars=bars_1m
            )
            
            # Calculate quality
            elapsed_minutes = int((current_time - session_start_time).total_seconds() / 60)
            if elapsed_minutes <= 0:
                quality = 100.0
            else:
                expected_bars = elapsed_minutes
                missing_bars = sum(g.bar_count for g in gaps)
                actual_bars = expected_bars - missing_bars
                quality = (actual_bars / expected_bars) * 100.0
            
            # Update in session_data (thread-safe)
            with self._session_data._lock:
                symbol_data.bar_quality = quality
                
        except Exception as e:
            logger.debug(f"Error updating quality for {symbol}: {e}")
    
    def feed_stream(
        self,
        symbol: str,
        stream_type: StreamType,
        data_iterator: Iterator[Any],
    ) -> None:
        """Feed data from an iterator into a registered stream.
        
        This is typically called by DataManager.stream_bars/ticks/quotes
        to push their database query results into the coordinator.
        
        Args:
            symbol: Stock symbol
            stream_type: Type of data
            data_iterator: Async iterator yielding data objects
        """
        stream_key = (symbol.upper(), stream_type)
        
        with self._lock:
            input_queue = self._active_streams.get(stream_key)
        
        if input_queue is None:
            logger.error(
                f"Cannot feed stream for {stream_key} - stream not registered"
            )
            return
        
        # Track timestamps as data flows through
        first_ts = None
        last_ts = None
        item_count = 0
        
        try:
            for data in data_iterator:
                # Put into thread-safe queue
                input_queue.put(data)
                
                # Track timestamps for queue statistics
                if hasattr(data, 'timestamp'):
                    ts = data.timestamp
                    if first_ts is None:
                        first_ts = ts
                    last_ts = ts  # Keep updating to get the newest
                    item_count += 1
            
            # Update timestamp tracking with data from this stream
            if first_ts and last_ts:
                with self._lock:
                    if stream_key in self._queue_timestamps:
                        # Initialize with first timestamp seen
                        if self._queue_timestamps[stream_key]["newest"] is None:
                            self._queue_timestamps[stream_key]["newest"] = first_ts
                        
                        # Update to keep the maximum (newest) timestamp
                        if last_ts > self._queue_timestamps[stream_key]["newest"]:
                            self._queue_timestamps[stream_key]["newest"] = last_ts
        
        finally:
            # Signal stream exhaustion
            input_queue.put(None)
            logger.info(f"Stream feed completed for {stream_key}: {item_count} items streamed")


# Global singleton coordinator instance
_coordinator: Optional[BacktestStreamCoordinator] = None


def get_coordinator(system_manager=None, data_manager=None) -> BacktestStreamCoordinator:
    """Get or create the global backtest stream coordinator instance.
    
    Args:
        system_manager: Reference to SystemManager (REQUIRED for production use)
                       The coordinator needs SystemManager to:
                       - Determine operation mode (live vs backtest)
                       - Check system state (running/paused/stopped)
                       - Pause time advancement when system is paused
        data_manager: Reference to DataManager for prefetch worker
    
    The coordinator will use TimeManager (via SystemManager), ensuring
    time is synchronized across all components.
    
    WARNING: If system_manager is None, the coordinator will log errors and
    cannot properly manage backtest time advancement or respect system state.
    """
    global _coordinator
    if _coordinator is None:
        _coordinator = BacktestStreamCoordinator(
            system_manager=system_manager,
            data_manager=data_manager
        )
    return _coordinator


def reset_coordinator() -> None:
    """Reset the global coordinator (useful for testing or reinitialization)."""
    global _coordinator
    if _coordinator is not None:
        _coordinator.stop_worker()
    _coordinator = None
