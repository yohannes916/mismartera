"""
================================================================================
DATA UPKEEP THREAD - Session Lifecycle & Data Quality Management
================================================================================

OVERVIEW:
This module implements a critical background thread that manages two main areas:

1. SESSION LIFECYCLE (Backtest Mode)
   - Detects end-of-day (EOD) based on market close time
   - Deactivates session at market close
   - Advances backtest time to next trading day
   - Activates session for new day
   - Coordinates with PrefetchWorker to load next day's data

2. DATA QUALITY MAINTENANCE (All Modes)
   - Calculates bar quality metrics (% of expected bars received)
   - Detects gaps in bar data using timeline analysis
   - Fills missing bars from Parquet storage
   - Computes derived bars (5m, 15m, 30m, etc. from 1m bars)
   - Creates bars from tick streams when needed (VWAP aggregation)

ARCHITECTURE:
┌─────────────────┐
│ SystemManager   │ ──> Provides TimeManager access, stop() control
└─────────────────┘
         │
         ▼
┌─────────────────────────────────────────────────────────────────┐
│                      DataUpkeepThread                            │
│                                                                  │
│  Main Loop (_run_upkeep_loop):                                  │
│  ├─ Check EOD → Deactivate → Advance → Activate → Prefetch     │
│  ├─ Check Initial Activation → Activate → Prefetch              │
│  ├─ Check Stream Exhaustion → Force advance to close            │
│  └─ Run Symbol Upkeep → Quality, Gaps, Derived                  │
│                                                                  │
│  Timing:                                                         │
│  - First cycle: 0.1s delay (instant feedback)                   │
│  - Regular: Event-driven with 1s timeout (frequent EOD check)   │
└─────────────────────────────────────────────────────────────────┘
         │                           │
         │                           ├──> PrefetchWorker (3rd thread)
         │                           │    └─> Loads historical bars
         │                           │
         ▼                           ▼
┌──────────────────┐      ┌──────────────────────────────┐
│  SessionData     │◄─────│ BacktestStreamCoordinator    │
│  (Thread-safe)   │      │ (Main data feed thread)      │
└──────────────────┘      └──────────────────────────────┘
   Shared via locks              Signals via events

THREAD COORDINATION:
- Shared Resource: session_data (protected by _lock)
- Event-Driven: _data_arrival_event wakes upkeep when new data arrives
- Time Source: TimeManager (single source of truth)
- Data Flow: Coordinator feeds → Upkeep analyzes → Quality metrics updated

CREATION & LIFECYCLE:
1. Created by: DataManager.start_bar_streams()
2. Started by: upkeep_thread.start()
3. Runs until: stop() called or system shutdown
4. Main method: _run_upkeep_loop() (infinite loop)

For detailed analysis, see: /backend/DATA_UPKEEP_THREAD_ANALYSIS.md
================================================================================
"""
import threading
import time
import asyncio
from datetime import datetime, timedelta, timezone
from typing import Optional, Dict, List, Set, Any
from collections import defaultdict

from app.config import settings
from app.logger import logger
from app.managers.data_manager.session_data import SessionData
from app.managers.data_manager.gap_detection import (
    detect_gaps,
    GapInfo,
    merge_overlapping_gaps
)
from app.managers.data_manager.quality_checker import calculate_session_quality
from app.managers.data_manager.derived_bars import compute_all_derived_intervals
from app.managers.data_manager.prefetch_worker import PrefetchWorker
from app.models.trading import BarData, TickData


class DataUpkeepThread:
    """Background thread for data quality maintenance.
    
    Responsibilities:
    - Detect gaps in 1-minute bar data
    - Fill missing bars from database
    - Compute derived bars (5m, 15m, etc.)
    - Update bar_quality metrics
    - Retry failed operations
    
    Thread-safe coordination with main coordinator thread via session_data locks.
    """
    
    def __init__(
        self,
        session_data: SessionData,
        system_manager,
        data_manager,
        data_repository=None
    ):
        """Initialize data-upkeep thread.
        
        Args:
            session_data: Reference to session_data singleton
            system_manager: Reference to SystemManager for mode/state checks
            data_manager: Reference to DataManager for data access
            data_repository: Data repository for fetching missing bars
        """
        self._session_data = session_data
        self._system_manager = system_manager
        self._data_manager = data_manager
        self._data_repository = data_repository
        
        # Get session config from SystemManager for config awareness
        self._session_config = system_manager._session_config if system_manager else None
        
        # Thread control
        self._thread: Optional[threading.Thread] = None
        self._shutdown = threading.Event()
        self._running = False
        
        # Configuration - prioritize session config over settings
        if self._session_config and self._session_config.session_data_config:
            upkeep_config = self._session_config.session_data_config.data_upkeep
            base_check_interval = upkeep_config.check_interval_seconds
            self._retry_enabled = upkeep_config.retry_missing_bars
            self._max_retries = upkeep_config.max_retries
            self._derived_intervals = upkeep_config.derived_intervals
            self._auto_compute_derived = upkeep_config.auto_compute_derived
        else:
            # Fallback to settings if no config
            base_check_interval = settings.DATA_UPKEEP_CHECK_INTERVAL_SECONDS
            self._retry_enabled = settings.DATA_UPKEEP_RETRY_MISSING_BARS
            self._max_retries = settings.DATA_UPKEEP_MAX_RETRIES
            self._derived_intervals = settings.DATA_UPKEEP_DERIVED_INTERVALS
            self._auto_compute_derived = settings.DATA_UPKEEP_AUTO_COMPUTE_DERIVED
        
        # Scale check interval based on backtest speed to catch EOD at high speeds
        # At 360x speed, checking every 10s means missing 3600 simulated seconds!
        speed_multiplier = 1.0
        if self._session_config and hasattr(self._session_config, 'backtest_config'):
            if self._session_config.backtest_config:
                speed_multiplier = max(self._session_config.backtest_config.speed_multiplier, 1.0)
        
        # Effective interval = base / speed, with minimum 0.1s to avoid hammering
        self._check_interval = max(base_check_interval / speed_multiplier, 0.1)
        
        if speed_multiplier > 1.0:
            logger.info(
                f"Upkeep check interval scaled for {speed_multiplier}x speed: "
                f"{base_check_interval}s → {self._check_interval:.3f}s "
                f"({base_check_interval}s simulated time between checks)"
            )
        
        # Build stream inventory from config (Phase 2: Stream Detection)
        self._stream_inventory = self._build_stream_inventory()
        logger.info(f"Stream inventory: {self._stream_inventory}")
        
        # Prefetch worker (3rd thread)
        self._prefetch_worker: Optional[PrefetchWorker] = PrefetchWorker(
            data_manager=data_manager,
            system_manager=system_manager
        )
        
        # Track failed gaps for retry
        self._failed_gaps: Dict[str, List] = {}
        
        # Track if session has been activated for current day
        self._session_activated_for_day = False
    
    def _build_stream_inventory(self) -> Dict[str, Dict[str, Any]]:
        """Build inventory of what data is available from streams.
        
        Returns:
            Dictionary mapping symbol to available stream types and intervals:
            {
                "AAPL": {
                    "bars": ["1m"],      # Bar intervals streamed
                    "ticks": False,       # Tick stream available?
                    "quotes": False       # Quote stream available?
                },
                "TSLA": {
                    "bars": [],
                    "ticks": True,
                    "quotes": False
                }
            }
        """
        inventory = {}
        
        if not self._session_config or not self._session_config.data_streams:
            logger.warning("No session config available, stream inventory empty")
            return inventory
        
        # Parse each stream in config
        for stream_config in self._session_config.data_streams:
            symbol = stream_config.symbol.upper()
            stream_type = stream_config.type
            
            # Initialize symbol entry if not exists
            if symbol not in inventory:
                inventory[symbol] = {
                    "bars": [],
                    "ticks": False,
                    "quotes": False
                }
            
            # Record what's available from streams
            if stream_type == "bars":
                if stream_config.interval:
                    inventory[symbol]["bars"].append(stream_config.interval)
            elif stream_type == "ticks":
                inventory[symbol]["ticks"] = True
            elif stream_type == "quotes":
                inventory[symbol]["quotes"] = True
        
        return inventory
    
    def _has_bar_stream(self, symbol: str, interval: str) -> bool:
        """Check if symbol has a bar stream for given interval.
        
        Args:
            symbol: Stock symbol
            interval: Bar interval (e.g., "1m", "5m")
            
        Returns:
            True if interval is streamed, False otherwise
        """
        symbol_upper = symbol.upper()
        if symbol_upper not in self._stream_inventory:
            return False
        
        return interval in self._stream_inventory[symbol_upper]["bars"]
    
    def _has_tick_stream(self, symbol: str) -> bool:
        """Check if symbol has a tick stream.
        
        Args:
            symbol: Stock symbol
            
        Returns:
            True if ticks are streamed, False otherwise
        """
        symbol_upper = symbol.upper()
        if symbol_upper not in self._stream_inventory:
            return False
        
        return self._stream_inventory[symbol_upper]["ticks"]
    
    def _has_quote_stream(self, symbol: str) -> bool:
        """Check if symbol has a quote stream.
        
        Args:
            symbol: Stock symbol
            
        Returns:
            True if quotes are streamed, False otherwise
        """
        symbol_upper = symbol.upper()
        if symbol_upper not in self._stream_inventory:
            return False
        
        return self._stream_inventory[symbol_upper]["quotes"]
    
    def _should_compute_derived_interval(self, symbol: str, interval: int) -> bool:
        """Check if derived interval should be computed for symbol.
        
        Derived intervals should NOT be computed if:
        1. The interval is already being streamed (avoid duplication)
        2. The interval is not in the configured derived_intervals list
        
        Args:
            symbol: Stock symbol
            interval: Interval in minutes (e.g., 5, 15)
            
        Returns:
            True if interval should be computed, False if streamed or not needed
        """
        # Check if this interval is in the derived list
        if interval not in self._derived_intervals:
            return False
        
        # Check if already streamed (avoid duplication)
        interval_str = f"{interval}m"
        if self._has_bar_stream(symbol, interval_str):
            logger.debug(
                f"{symbol} {interval_str}: Already streamed, skipping computation"
            )
            return False
        
        return True
    
    def _get_base_interval_for_symbol(self, symbol: str) -> str:
        """Get base interval for symbol from stream inventory.
        
        Args:
            symbol: Stock symbol
            
        Returns:
            Base interval string (e.g., "1m", "1s"), defaults to "1m"
        """
        symbol_upper = symbol.upper()
        if symbol_upper not in self._stream_inventory:
            return "1m"
        
        bars = self._stream_inventory[symbol_upper]["bars"]
        if not bars:
            return "1m"
        
        # Return the first (and should be only) bar interval
        # Session config validation ensures only 1s or 1m
        return bars[0]
    
    def _get_prefetch_interval(self) -> str:
        """Get interval for prefetch operations from stream inventory.
        
        Returns the most common base interval across all symbols.
        Defaults to "1m" if no inventory available.
        
        Returns:
            Interval string (e.g., "1m", "1s")
        """
        if not self._stream_inventory:
            return "1m"
        
        # Count intervals across symbols
        interval_counts = {}
        for symbol_inv in self._stream_inventory.values():
            for interval in symbol_inv["bars"]:
                interval_counts[interval] = interval_counts.get(interval, 0) + 1
        
        if not interval_counts:
            return "1m"
        
        # Return most common interval
        return max(interval_counts, key=interval_counts.get)
    
    def start(self) -> None:
        """Start the upkeep thread."""
        if self._running:
            logger.warning("DataUpkeepThread already running")
            return
        
        self._shutdown.clear()
        self._running = True
        
        self._thread = threading.Thread(
            target=self._upkeep_worker,
            name="DataUpkeep",
            daemon=True
        )
        self._thread.start()
        
        logger.info("DataUpkeepThread started with prefetch worker")
    
    def stop(self, timeout: float = 5.0) -> None:
        """Stop the upkeep thread.
        
        Args:
            timeout: Maximum seconds to wait for thread to stop
        """
        if not self._running:
            return
        
        logger.info("Stopping DataUpkeepThread...")
        
        self._shutdown.set()
        
        # Shutdown prefetch worker
        if self._prefetch_worker:
            self._prefetch_worker.shutdown()
        
        if self._thread:
            self._thread.join(timeout=timeout)
            if self._thread.is_alive():
                logger.warning("DataUpkeepThread did not stop within timeout")
            else:
                logger.info("DataUpkeepThread stopped")
        
        self._running = False
    
    def _upkeep_worker(self) -> None:
        """Main worker loop for upkeep thread.
        
        Manages session lifecycle and data quality maintenance.
        """
        logger.info("DataUpkeepThread worker started")
        
        try:
            # Run the upkeep loop (now sync)
            self._run_upkeep_loop()
        except Exception as e:
            logger.critical(f"DataUpkeepThread crashed: {e}", exc_info=True)
        finally:
            logger.info("DataUpkeepThread worker exiting")
    
    def _run_upkeep_loop(self) -> None:
        """
        ========================================================================
        MAIN UPKEEP LOOP - Session Lifecycle & Data Quality Management
        ========================================================================
        
        This is the heart of the upkeep thread. It runs continuously and handles:
        
        1. END-OF-DAY (EOD) DETECTION & TRANSITIONS
           - Monitors current time vs market close
           - Deactivates session at market close
           - Advances time to next trading day
           - Activates session for new day
           - Launches prefetch for new day's data
        
        2. INITIAL SESSION ACTIVATION
           - Detects when market opens for the first time
           - Activates session for the current trading day
           - Launches prefetch for historical data
        
        3. STREAM EXHAUSTION HANDLING
           - Detects if data streams end before market close
           - Forces time advancement to trigger EOD
        
        4. REGULAR DATA QUALITY TASKS
           - Bar quality calculation
           - Gap detection and filling
           - Derived bar computation
        
        Loop Structure:
        ├─ Check EOD (current_time >= close_time)
        │  └─> Handle day transition
        ├─ Check Initial Activation (!activated && current_time >= open_time)
        │  └─> Activate and prefetch
        ├─ Check Stream Exhaustion (active but no streams)
        │  └─> Force advance to close
        └─ Run Symbol Upkeep (quality, gaps, derived)
        
        Timing:
        - First cycle: 0.1s delay for instant feedback
        - Regular cycles: Event-driven with 1s timeout
        - Ensures EOD detected within 1 second
        ========================================================================
        """
        from app.models.database import SessionLocal
        
        logger.info("Session lifecycle management started")
        logger.info(f"Session initially active: {self._session_data.is_session_active()}")
        
        loop_count = 0
        first_cycle = True  # Run quality check immediately on first cycle
        
        while not self._shutdown.is_set():
            try:
                loop_count += 1
                
                # Log every 10th iteration with key metrics
                if loop_count % 10 == 1:
                    active_count = len(self._session_data.get_active_symbols())
                    logger.debug(
                        f"Upkeep cycle {loop_count}: session_active={self._session_data.is_session_active()}, "
                        f"symbols={active_count}, thread_alive=True"
                    )
                
                with SessionLocal() as db_session:
                    time_mgr = self._system_manager.get_time_manager()
                    # Get current time in system timezone
                    current_time = time_mgr.get_current_time()
                    current_date = current_time.date()
                    
                    # Get trading session for current market date
                    trading_session = time_mgr.get_trading_session(db_session, current_date)
                    
                    logger.debug(
                        f"EOD: current_time={current_time}, current_date={current_date}, "
                        f"trading_session.date={trading_session.date if trading_session else None}, "
                        f"trading_session.regular_close={trading_session.regular_close if trading_session else None}"
                    )
                    
                    if not trading_session or trading_session.is_holiday:
                        logger.debug(f"No trading session for {current_date}")
                        time.sleep(1.0)
                        continue
                    
                    # Get market close time in system timezone
                    close_time = trading_session.get_regular_close_datetime()
                    logger.debug(
                        f"EOD: close_time={close_time} (tz={close_time.tzinfo if close_time else None})"
                    )
                    if close_time is None:
                        logger.debug(f"No market close time for {current_date}")
                        time.sleep(1.0)
                        continue
                    
                    # Debug logging for EOD detection (every 10th cycle)
                    if loop_count % 10 == 1:
                        logger.debug(
                            f"EOD check: current={current_time} (tz={current_time.tzinfo}), "
                            f"close={close_time} (tz={close_time.tzinfo}), "
                            f"past_close={current_time >= close_time}"
                        )
                    
                    # ============================================================
                    # SECTION 1: END-OF-DAY (EOD) DETECTION & DAY TRANSITION
                    # ============================================================
                    # When: current_time >= market_close
                    # Actions:
                    #   1. Deactivate current session
                    #   2. Check if backtest window complete
                    #   3. Advance to next trading day's market open
                    #   4. Activate session for new day
                    #   5. Launch prefetch for new day's data
                    #   6. Skip to next cycle (continue)
                    # ============================================================
                    if current_time >= close_time:
                        logger.info(f"EOD: Market close reached at {current_time}")
                        
                        # 1. Deactivate session
                        self._session_data.deactivate_session()
                        self._session_activated_for_day = False
                        
                        # 2. Check if backtest complete
                        if current_date >= time_mgr.backtest_end_date:
                            logger.info("Backtest window complete, stopping system")
                            self._system_manager.stop()
                            break
                        
                        # 3. Advance to next trading day's market open
                        # TimeManager handles skipping weekends/holidays automatically
                        next_open = time_mgr.advance_to_market_open(db_session)
                        next_date = next_open.date()
                        logger.info(f"Advanced to next market open: {next_date} @ {next_open.strftime('%H:%M:%S')}")
                        
                        # 4. Activate session for new day
                        self._session_data.activate_session()
                        self._session_activated_for_day = True
                        logger.info(f"Session activated for {next_date}")
                        
                        # 5. Launch prefetch for next day (full day: open to close)
                        symbols = list(self._session_data.get_active_symbols())
                        interval = self._get_prefetch_interval()
                        
                        if symbols:
                            prefetch_future = self._prefetch_worker.start_prefetch(
                                target_date=next_date,
                                symbols=symbols,
                                interval=interval,
                                start_time=None  # Load full day
                            )
                            logger.info(f"Prefetch launched for {next_date}")
                        
                        # Skip to next cycle after handling EOD
                        continue
                    
                    # ============================================================
                    # SECTION 2: INITIAL SESSION ACTIVATION
                    # ============================================================
                    # When: !session_activated_for_day && current_time >= market_open
                    # Actions:
                    #   1. Activate session for current trading day
                    #   2. Launch prefetch for historical data
                    #   3. Wait for prefetch to complete (blocking)
                    # Purpose: Handles mid-day system starts or first day of backtest
                    # ============================================================
                    elif not self._session_activated_for_day:
                        # Get market open time from TradingSession
                        open_time = trading_session.get_regular_open_datetime()
                        if open_time is None:
                            logger.debug(f"No market open time for {current_date}")
                            time.sleep(1.0)
                            continue
                        
                        logger.debug(
                            f"Checking initial activation: current={current_time}, "
                            f"open={open_time}, activated={self._session_activated_for_day}"
                        )
                        
                        if current_time >= open_time:
                            logger.info(f"Initial activation for {current_date}")
                            
                            # Activate session
                            self._session_data.activate_session()
                            self._session_activated_for_day = True
                            
                            # Launch prefetch for current day (mid-session start: open to current_time)
                            symbols = list(self._session_data.get_active_symbols())
                            interval = self._get_prefetch_interval()
                            
                            if symbols:
                                prefetch_future = self._prefetch_worker.start_prefetch(
                                    target_date=current_date,
                                    symbols=symbols,
                                    interval=interval,
                                    start_time=current_time  # Load up to current time
                                )
                                logger.info(f"Initial prefetch launched for {current_date} up to {current_time}")
                                
                                # Wait for initial prefetch to complete (blocking)
                                success = self._prefetch_worker.wait_for_completion(timeout=30.0)
                                if not success:
                                    logger.error("Initial prefetch failed or timed out")
                                else:
                                    logger.info("Initial prefetch completed successfully")
                    
                    # ============================================================
                    # SECTION 3: STREAM EXHAUSTION HANDLING
                    # ============================================================
                    # When: session_active && no_active_streams && before_close
                    # Action: Force advance time to market close to trigger EOD
                    # Purpose: Handles cases where all data streams end mid-day
                    #          (e.g., parquet data exhausted, no live feed)
                    # ============================================================
                    if self._session_data.is_session_active():
                        active_symbols = self._session_data.get_active_symbols()
                        if active_symbols:
                            # Check if we have data for today but no streams are active
                            # This means streams exhausted mid-day
                            from app.managers.data_manager.backtest_stream_coordinator import BacktestStreamCoordinator
                            
                            # Get coordinator from data_manager
                            coordinator = None
                            if self._data_manager and hasattr(self._data_manager, '_stream_coordinator'):
                                coordinator = self._data_manager._stream_coordinator
                            
                            if coordinator and isinstance(coordinator, BacktestStreamCoordinator):
                                # Check if coordinator has any active streams
                                with coordinator._lock:
                                    has_active_streams = len(coordinator._active_streams) > 0
                                
                                # If no active streams AND we're before market close
                                # Force advance time to trigger EOD
                                if not has_active_streams and current_time < close_time:
                                    logger.info(
                                        f"Streams exhausted before market close. "
                                        f"Force-advancing from {current_time.strftime('%H:%M:%S')} "
                                        f"to {close_time.strftime('%H:%M:%S')}"
                                    )
                                    time_mgr.set_backtest_time(close_time)
                                    # Let next iteration handle EOD sequence
                                    continue
                    
                    # ============================================================
                    # SECTION 4: REGULAR DATA QUALITY UPKEEP
                    # ============================================================
                    # When: session_active (during normal trading hours)
                    # Actions: For each active symbol:
                    #   1. Calculate bar quality (% of expected bars received)
                    #   2. Detect and fill gaps in bar data
                    #   3. Compute derived bars (5m, 15m from 1m)
                    # ============================================================
                    if self._session_data.is_session_active():
                        self._run_symbol_upkeep(db_session)
                
            except Exception as e:
                logger.error(f"Error in upkeep loop: {e}", exc_info=True)
            
            # Wait for new data or timeout
            # Event-driven: wakes immediately when data arrives
            # Timeout fallback: wakes frequently (1s) for EOD detection even if no data
            if first_cycle:
                first_cycle = False
                timeout = 0.1  # Short timeout on first cycle
            else:
                # Use 1 second timeout for frequent EOD checks
                # This ensures we detect market close quickly (within 1 second)
                timeout = 1.0
            
            # Wait for data arrival event or timeout
            self._session_data._data_arrival_event.wait(timeout=timeout)
            # Clear event for next cycle
            self._session_data._data_arrival_event.clear()
        
        logger.info("Session lifecycle management ended")
    
    def _run_symbol_upkeep(self, db_session) -> None:
        """
        Run upkeep tasks for all active symbols.
        
        Args:
            db_session: Database session for queries
        """
        active_symbols = self._session_data.get_active_symbols()
        
        if not active_symbols:
            return
        
        for symbol in active_symbols:
            try:
                self._upkeep_symbol(symbol)
            except Exception as e:
                logger.error(f"Error in upkeep for {symbol}: {e}")
    
    def _should_run_upkeep(self) -> bool:
        """Check if upkeep should run based on system state.
        
        Returns:
            True if upkeep should run
        """
        if self._system_manager is None:
            return False
        
        # Only run in backtest mode when system is running
        if self._system_manager.is_backtest_mode():
            return self._system_manager.is_running()
        
        # In live mode, always run
        return True
    
    def _run_upkeep_cycle(self) -> None:
        """Run one cycle of upkeep tasks."""
        # Get active symbols
        active_symbols = self._session_data.get_active_symbols()
        
        if not active_symbols:
            logger.debug("No active symbols, skipping upkeep")
            return
        
        logger.debug(f"Running upkeep cycle for {len(active_symbols)} symbols")
        
        # Run upkeep for each symbol (synchronous)
        for symbol in active_symbols:
            try:
                self._upkeep_symbol(symbol)
            except Exception as e:
                logger.error(f"Error in upkeep for {symbol}: {e}", exc_info=True)
    
    def _upkeep_symbol(self, symbol: str) -> None:
        """
        ====================================================================
        SYMBOL UPKEEP - Data Quality Tasks for Individual Symbol
        ====================================================================
        
        Performs 4 sequential tasks for each active symbol:
        
        0. ENSURE BASE BARS (Prerequisite)
           - Check if symbol has 1m or 1s bar stream
           - If not, create bars from tick stream (VWAP aggregation)
           - Skip upkeep if no base data available
        
        1. UPDATE BAR QUALITY
           - Calculate quality = (actual_bars / expected_bars) * 100
           - Detect gaps in bar data using gap_detection module
           - Update symbol_data.bar_quality metric
        
        2. CHECK AND FILL GAPS (if retry enabled)
           - Detect gaps from session start to current time
           - Attempt to fill from Parquet storage
           - Track failed gaps for retry
        
        3. UPDATE DERIVED BARS (if auto_compute_derived enabled)
           - Compute 5m, 15m, 30m, etc. from 1m bars
           - Skip intervals already being streamed
           - Update symbol_data.bars_derived
        
        Args:
            symbol: Stock symbol
        ====================================================================
        """
        # Get symbol data
        symbol_data = self._session_data.get_symbol_data(symbol)
        if symbol_data is None:
            return
        
        # ----------------------------------------------------------------
        # STEP 0: Ensure base bars exist (create from ticks if needed)
        # This must be done first before quality checks or derived computation
        # ----------------------------------------------------------------
        base_bars_available = self._ensure_base_bars(symbol)
        if not base_bars_available:
            # No base data source available, skip upkeep
            logger.debug(f"{symbol}: No base data available, skipping upkeep")
            return
        
        # Skip if no data yet (after ensuring base bars)
        if len(symbol_data.bars_1m) == 0:
            return
        
        # ----------------------------------------------------------------
        # STEP 1: Calculate and update bar quality metric
        # ----------------------------------------------------------------
        self._update_bar_quality(symbol)
        
        # ----------------------------------------------------------------
        # STEP 2: Detect and fill gaps in bar data (if enabled)
        # ----------------------------------------------------------------
        if self._retry_enabled:
            self._check_and_fill_gaps(symbol)
        
        # ----------------------------------------------------------------
        # STEP 3: Compute derived bars from 1m bars (if enabled)
        # ----------------------------------------------------------------
        if self._auto_compute_derived and symbol_data.bars_updated:
            self._update_derived_bars(symbol)
    
    def _update_bar_quality(self, symbol: str) -> None:
        """Update bar quality metric for a symbol using detailed gap analysis.
        
        Quality is calculated based on session_data bars (consumed bars) from
        session open to current time. This informs gap-filling logic.
        
        Args:
            symbol: Stock symbol
        """
        symbol_data = self._session_data.get_symbol_data(symbol)
        if symbol_data is None:
            return
        
        # Get time manager for current time (single source of truth)
        time_mgr = self._system_manager.get_time_manager()
        
        try:
            # Get current time in system timezone
            current_time = time_mgr.get_current_time()
            current_date = current_time.date()
        except ValueError:
            # Backtest time not set yet
            return
        
        # Get trading hours from time_manager (single source of truth)
        # This accounts for holidays, early closes, etc.
        from app.models.database import SessionLocal
        with SessionLocal() as session:
            trading_session = time_mgr.get_trading_session(session, current_date)
            
            if trading_session is None or trading_session.is_holiday:
                # Market closed today (holiday)
                return
            
            # Get session start time from TradingSession
            session_start_time = trading_session.get_regular_open_datetime()
            if session_start_time is None:
                return
        
        # Use detailed gap analysis from session_start to current_time
        # This analyzes what we HAVE consumed vs what we SHOULD have
        bars_1m = list(symbol_data.bars_1m)
        gaps = detect_gaps(
            symbol=symbol,
            session_start=session_start_time,
            current_time=current_time,
            existing_bars=bars_1m
        )
        
        # Calculate expected minutes from session start to current time
        elapsed_minutes = int((current_time - session_start_time).total_seconds() / 60)
        
        if elapsed_minutes <= 0:
            quality = 100.0  # Session hasn't started yet
        else:
            # Quality = (actual bars / expected bars) * 100
            expected_bars = elapsed_minutes
            missing_bars = sum(g.bar_count for g in gaps)
            actual_bars = expected_bars - missing_bars
            quality = (actual_bars / expected_bars) * 100.0
        
        # Debug logging
        logger.info(
            f"{symbol} quality: {quality:.1f}% | "
            f"bars={len(bars_1m)}, expected={elapsed_minutes}, "
            f"gaps={len(gaps)} ({sum(g.bar_count for g in gaps)} missing)"
        )
        
        # Update in session_data
        with self._session_data._lock:
            symbol_data.bar_quality = quality
    
    def _check_and_fill_gaps(self, symbol: str) -> None:
        """Detect gaps and attempt to fill them.
        
        Args:
            symbol: Stock symbol
        """
        symbol_data = self._session_data.get_symbol_data(symbol)
        if symbol_data is None:
            return
        
        # Get current time (TimeManager is single source of truth)
        time_mgr = self._system_manager.get_time_manager()
        
        try:
            # Get current time in system timezone
            current_time = time_mgr.get_current_time()
        except ValueError:
            return
        
        # Get trading hours from time_manager (single source of truth)
        # This accounts for holidays, early closes, etc.
        current_date = current_time.date()
        
        from app.models.database import SessionLocal
        with SessionLocal() as session:
            trading_session = time_mgr.get_trading_session(session, current_date)
            
            if trading_session is None or trading_session.is_holiday:
                # Market closed today (holiday)
                return
            
            # Get session start time in system timezone
            session_start_time = trading_session.get_regular_open_datetime()
        
        # Detect gaps
        bars_1m = list(symbol_data.bars_1m)
        gaps = detect_gaps(
            symbol=symbol,
            session_start=session_start_time,
            current_time=current_time,
            existing_bars=bars_1m
        )
        
        # Add previously failed gaps for retry
        if symbol in self._failed_gaps:
            gaps.extend(self._failed_gaps[symbol])
            gaps = merge_overlapping_gaps(gaps)
        
        if not gaps:
            return
        
        # Attempt to fill gaps
        filled_count = 0
        remaining_gaps = []
        
        for gap in gaps:
            if gap.retry_count >= self._max_retries:
                logger.warning(f"Max retries reached for gap: {gap}")
                continue
            
            # Try to fill this gap
            try:
                count = self._fill_gap(symbol, gap)
                filled_count += count
                
                if count < gap.bar_count:
                    # Partial fill or failure
                    gap.retry_count += 1
                    gap.last_retry = time_mgr.get_current_time()
                    remaining_gaps.append(gap)
            
            except Exception as e:
                logger.error(f"Error filling gap: {e}")
                gap.retry_count += 1
                remaining_gaps.append(gap)
        
        # Update failed gaps
        self._failed_gaps[symbol] = remaining_gaps
        
        if filled_count > 0:
            logger.info(f"Filled {filled_count} bars for {symbol}")
    
    def _fill_gap(self, symbol: str, gap: GapInfo) -> int:
        """Fill a single gap by fetching from database.
        
        Args:
            symbol: Stock symbol
            gap: Gap to fill
            
        Returns:
            Number of bars filled
        """
        if self._data_repository is None:
            logger.debug(f"No data_repository available, skipping gap fill for {symbol}")
            return 0
        
        logger.debug(f"Attempting to fill gap: {gap}")
        
        try:
            # Fetch bars from database
            # The data_repository should have a method like get_bars_by_symbol
            # which queries the market data database
            
            # Load bars from Parquet (no database)
            from app.managers.data_manager.parquet_storage import parquet_storage
            
            df = parquet_storage.read_bars(
                "1m",
                symbol,
                start_date=gap.start_time,
                end_date=gap.end_time
            )
            
            # Convert DataFrame to list of BarData objects
            bars_db = []
            if not df.empty:
                for _, row in df.iterrows():
                    bars_db.append(BarData(
                        symbol=row['symbol'],
                        timestamp=row['timestamp'],
                        interval=row.get('interval', '1m'),
                        open=row['open'],
                        high=row['high'],
                        low=row['low'],
                        close=row['close'],
                        volume=row['volume']
                    ))
            
            if not bars_db:
                logger.debug(f"No bars found in database for gap: {gap}")
                return 0
            
            # Convert database bars to BarData format
            from app.models.trading import BarData
            bars_to_insert = []
            
            for db_bar in bars_db:
                # Handle different possible database schemas
                try:
                    bar = BarData(
                        symbol=symbol,
                        timestamp=db_bar.timestamp,
                        open=float(db_bar.open),
                        high=float(db_bar.high),
                        low=float(db_bar.low),
                        close=float(db_bar.close),
                        volume=int(db_bar.volume) if db_bar.volume else 0
                    )
                    bars_to_insert.append(bar)
                except (AttributeError, TypeError, ValueError) as e:
                    logger.error(f"Failed to convert database bar to BarData: {e}")
                    continue
            
            if not bars_to_insert:
                logger.warning(f"No valid bars to insert for gap: {gap}")
                return 0
            
            # Insert bars into session_data using gap_fill mode to maintain chronological order
            self._session_data.add_bars_batch(symbol, bars_to_insert, insert_mode="gap_fill")
            
            logger.info(
                f"Successfully filled {len(bars_to_insert)} bars for {symbol} "
                f"from {gap.start_time} to {gap.end_time}"
            )
            
            return len(bars_to_insert)
        
        except Exception as e:
            logger.error(
                f"Error filling gap for {symbol} ({gap.start_time} to {gap.end_time}): {e}",
                exc_info=True
            )
            return 0
    
    def _update_derived_bars(self, symbol: str) -> None:
        """Recompute derived bars when 1m bars are updated.
        
        Priority: Compute each interval as soon as enough 1m bars are available.
        This allows 5m bars to be computed before 15m bars, providing faster feedback.
        
        Args:
            symbol: Stock symbol
        """
        symbol_data = self._session_data.get_symbol_data(symbol)
        if symbol_data is None or not symbol_data.bars_updated:
            return
        
        # Get 1m bars
        bars_1m = list(symbol_data.bars_1m)
        total_bars = len(bars_1m)
        
        if total_bars == 0:
            return
        
        # Compute derived bars for each interval independently
        # PRIORITY: Compute as soon as we have enough bars for that specific interval
        # STREAM DETECTION: Skip intervals already being streamed
        derived_bars = {}
        intervals_computed = []
        intervals_skipped = []
        
        for interval in sorted(self._derived_intervals):  # Process in ascending order
            # Check if should compute (not streamed, in config)
            if not self._should_compute_derived_interval(symbol, interval):
                intervals_skipped.append(interval)
                continue
            
            if total_bars >= interval:
                # We have enough bars for this interval
                bars_for_interval = compute_all_derived_intervals(bars_1m, [interval])
                if interval in bars_for_interval:
                    derived_bars[interval] = bars_for_interval[interval]
                    intervals_computed.append(interval)
        
        if not derived_bars:
            # Not enough bars for even the smallest interval
            return
        
        # Update in session_data
        with self._session_data._lock:
            for interval, bars in derived_bars.items():
                symbol_data.bars_derived[interval] = bars
            
            # Reset update flag
            symbol_data.bars_updated = False
        
        logger.debug(
            f"Updated derived bars for {symbol} ({total_bars} 1m bars): "
            f"{', '.join(f'{k}m={len(v)} bars' for k, v in derived_bars.items())}"
        )
    
    def _create_bars_from_ticks(
        self, 
        symbol: str, 
        interval: str,
        up_to_time: Optional[datetime] = None
    ) -> int:
        """Create bar data from tick stream using VWAP aggregation.
        
        This is called when a symbol has tick stream but no bar stream for
        the required interval. Uses Volume-Weighted Average Price (VWAP)
        for better price representation.
        
        Args:
            symbol: Stock symbol
            interval: Target interval ("1s" or "1m")
            up_to_time: Optional cutoff time (only aggregate ticks up to this time)
            
        Returns:
            Number of bars created
        """
        symbol_data = self._session_data.get_symbol_data(symbol)
        if symbol_data is None:
            return 0
        
        ticks = list(symbol_data.ticks)
        if not ticks:
            logger.debug(f"{symbol}: No ticks available for bar creation")
            return 0
        
        # Determine interval duration
        if interval == "1s":
            bar_duration = timedelta(seconds=1)
        elif interval == "1m":
            bar_duration = timedelta(minutes=1)
        else:
            logger.error(f"Unsupported interval for tick-to-bar: {interval}")
            return 0
        
        # Get current time limit
        if up_to_time is None:
            time_mgr = self._system_manager.get_time_manager()
            up_to_time = time_mgr.get_current_time()
        
        # Group ticks by interval
        # Key: bar start timestamp, Value: list of ticks
        tick_groups: Dict[datetime, List[TickData]] = defaultdict(list)
        
        for tick in ticks:
            # Skip ticks beyond cutoff time
            if tick.timestamp > up_to_time:
                continue
            
            # Calculate bar start time (floor to interval boundary)
            if interval == "1s":
                # Floor to second
                bar_start = tick.timestamp.replace(microsecond=0)
            else:  # 1m
                # Floor to minute
                bar_start = tick.timestamp.replace(second=0, microsecond=0)
            
            tick_groups[bar_start].append(tick)
        
        if not tick_groups:
            logger.debug(f"{symbol}: No ticks to aggregate into {interval} bars")
            return 0
        
        # Aggregate ticks into bars using VWAP
        bars_created = []
        
        for bar_start in sorted(tick_groups.keys()):
            ticks_in_bar = tick_groups[bar_start]
            
            # Calculate OHLC using VWAP
            prices = [t.price for t in ticks_in_bar]
            sizes = [t.size for t in ticks_in_bar]
            
            # VWAP calculation
            total_volume = sum(sizes)
            if total_volume > 0:
                vwap = sum(p * s for p, s in zip(prices, sizes)) / total_volume
            else:
                vwap = prices[0]  # Fallback to first price
            
            # OHLC
            open_price = prices[0]   # First tick
            high_price = max(prices)
            low_price = min(prices)
            close_price = prices[-1]  # Last tick
            
            # Create bar
            bar = BarData(
                symbol=symbol,
                timestamp=bar_start,
                open=open_price,
                high=high_price,
                low=low_price,
                close=close_price,
                volume=int(total_volume),
                interval=interval
            )
            bars_created.append(bar)
        
        if not bars_created:
            return 0
        
        # Add bars to session_data
        # Use batch insert for efficiency
        self._session_data.add_bars_batch(
            symbol, 
            bars_created, 
            insert_mode="chronological"
        )
        
        logger.info(
            f"{symbol}: Created {len(bars_created)} {interval} bars from "
            f"{len(ticks)} ticks (VWAP aggregation)"
        )
        
        return len(bars_created)
    
    def _ensure_base_bars(self, symbol: str) -> bool:
        """Ensure base interval bars exist for symbol.
        
        Checks stream inventory and creates bars from ticks if needed.
        This is called before computing derived bars to ensure base data exists.
        
        Args:
            symbol: Stock symbol
            
        Returns:
            True if base bars are available (streamed or created), False otherwise
        """
        # Check what's available
        has_1m_stream = self._has_bar_stream(symbol, "1m")
        has_1s_stream = self._has_bar_stream(symbol, "1s")
        has_tick_stream = self._has_tick_stream(symbol)
        
        # If we have bar stream (1m or 1s), we're good
        if has_1m_stream or has_1s_stream:
            return True
        
        # No bar stream - check if we can create from ticks
        if has_tick_stream:
            logger.info(
                f"{symbol}: No bar stream, creating 1m bars from tick stream"
            )
            
            # Create 1m bars from ticks
            bars_created = self._create_bars_from_ticks(symbol, "1m")
            
            if bars_created > 0:
                logger.success(
                    f"{symbol}: Successfully created {bars_created} bars from ticks"
                )
                return True
            else:
                logger.warning(
                    f"{symbol}: Tick stream available but no bars created"
                )
                return False
        
        # No bar stream and no tick stream
        logger.warning(
            f"{symbol}: No bar stream or tick stream available for base data"
        )
        return False
    
    def get_status(self) -> Dict[str, any]:
        """Get current status of upkeep thread.
        
        Returns:
            Dictionary with status information
        """
        return {
            "running": self._running,
            "check_interval": self._check_interval,
            "derived_intervals": self._derived_intervals,
            "failed_gaps_count": sum(len(gaps) for gaps in self._failed_gaps.values()),
            "symbols_with_failed_gaps": list(self._failed_gaps.keys())
        }


# Singleton instance management
_upkeep_thread_instance: Optional[DataUpkeepThread] = None


def get_upkeep_thread(
    session_data: SessionData,
    system_manager,
    data_repository=None
) -> DataUpkeepThread:
    """Get or create the global DataUpkeepThread instance.
    
    Args:
        session_data: SessionData singleton
        system_manager: SystemManager reference
        data_repository: Optional data repository
        
    Returns:
        DataUpkeepThread instance
    """
    global _upkeep_thread_instance
    if _upkeep_thread_instance is None:
        _upkeep_thread_instance = DataUpkeepThread(
            session_data,
            system_manager,
            data_repository
        )
    return _upkeep_thread_instance


def reset_upkeep_thread() -> None:
    """Reset the global upkeep thread instance (for testing)."""
    global _upkeep_thread_instance
    if _upkeep_thread_instance and _upkeep_thread_instance._running:
        _upkeep_thread_instance.stop()
    _upkeep_thread_instance = None
